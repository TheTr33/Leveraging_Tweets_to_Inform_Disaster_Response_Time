{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Data Gathering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll show you how we've gathered both historical with GetOldTweets3, and current tweets with Tweepy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Install and Import Packages](#Import-and-Import-Packages)\n",
    "- [Historical Twitter Data](#Historical-Twitter-Data)\n",
    "- [Current Tweets](#Current-Tweets)\n",
    "- [Sources](#Sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you'll need to install these packages if they aren't already installed. \n",
    "#!pip install GetOldTweets3\n",
    "#!pip install Tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import GetOldTweets3 as got\n",
    "import time\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the GetOldTweets3 library to pull historical twitter data. We build a function to use GetOldTweets3 to pull a list of search words related to the disaster. Hurricane Michael began to develop on October 1st, 2018, so that's when we began our search and scrapped through October 15th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we began by creating a dictionary to store tweet infromation in. adapted from this project: \n",
    "# https://github.com/giffordtompkinsiii/Evacuation_Twitter_Client_Project/blob/master/code/01_Historic_and_Live_Twitter_Scraper.ipynb\n",
    "def create_tweet(tweet):    \n",
    "    tweet_dict = {}\n",
    "    tweet_dict['id'] = tweet.id\n",
    "    tweet_dict['username'] = tweet.username\n",
    "    tweet_dict['date'] = tweet.date\n",
    "    tweet_dict['text'] = tweet.text\n",
    "    tweet_dict['hashtags'] = tweet.hashtags\n",
    "    tweet_dict['geo'] = tweet.geo\n",
    "    return tweet_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to pull tweets using GetOldTweets3 \n",
    "def get_tweets(search_criteria,start_date, end_date, tweets_per_search):\n",
    "    tweet_list = []\n",
    "    for word in search_criteria: # queries for word in list of words to search\n",
    "            tweetCriteria = got.manager.TweetCriteria().setQuerySearch(word)\\\n",
    "                                                       .setSince(start_date)\\\n",
    "                                                       .setUntil(end_date)\\\n",
    "                                                       .setMaxTweets(tweets_per_search)\\\n",
    "\n",
    "            tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "            time.sleep(2)\n",
    "\n",
    "            for tweet in tweets:\n",
    "                tweet_list.append(create_tweet(tweet))\n",
    "    return pd.DataFrame(tweet_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, compile a list of words to use as your search parameters. Since many tweets don't have any attached location data, the more specific these terms are, the better. We recommend having a pre-built list with popular hashtags used to identify the area affected and any terms that might be used to identify the event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of words to include in our GetOldTweets3 query. \n",
    "search_words = ['hurricanemichael','hurricane', 'panamacitybeach', 'panamacityflorida', '850strong', 'gulfpower',\n",
    "                'baycounty', 'floridastrong', 'panhandlestrong']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there we used the function to pull tweets, then check for duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the function to pull tweets, Oct. 1 - Oct. 15st 2018, returning 500 tweets per searchterm \n",
    "test = get_tweets(search_words, '2018-10-01' , '2018-10-15', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3790, 6)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking out the shape \n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping any duplicates. \n",
    "test.drop_duplicates(subset = 'id', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3658, 6)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#re-checking shape, we've droppped 132 duplicate tweets \n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>username</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>geo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1051623671951974400</td>\n",
       "      <td>GulfPower</td>\n",
       "      <td>2018-10-14 23:59:56+00:00</td>\n",
       "      <td>â€œWe are pleased to be making steady progress a...</td>\n",
       "      <td>#HurricaneMichael</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1051623669363965952</td>\n",
       "      <td>Postcards4Potus</td>\n",
       "      <td>2018-10-14 23:59:55+00:00</td>\n",
       "      <td>@realDonaldTrump really doesn't care! Seriousl...</td>\n",
       "      <td>#HurricaneMichael</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1051623651320184832</td>\n",
       "      <td>LauraHKByrne</td>\n",
       "      <td>2018-10-14 23:59:51+00:00</td>\n",
       "      <td>This is an excellent point. The best and easie...</td>\n",
       "      <td>#HurricaneMichael</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1051623649197875201</td>\n",
       "      <td>SupplierCom</td>\n",
       "      <td>2018-10-14 23:59:50+00:00</td>\n",
       "      <td>For those affected by #HurricaneMichael member...</td>\n",
       "      <td>#HurricaneMichael</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1051623615911927808</td>\n",
       "      <td>Heart_to_Heart</td>\n",
       "      <td>2018-10-14 23:59:42+00:00</td>\n",
       "      <td>The devastation from #HurricaneMichael is hard...</td>\n",
       "      <td>#HurricaneMichael #PanamaCity #Florida</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3785</th>\n",
       "      <td>1050007027676835842</td>\n",
       "      <td>valerie__lyn</td>\n",
       "      <td>2018-10-10 12:55:58+00:00</td>\n",
       "      <td>Praying for my home and everyone in the panhan...</td>\n",
       "      <td>#FloridaStrong #PanhandleStrong #HurricaneMichael</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3786</th>\n",
       "      <td>1050005160351666176</td>\n",
       "      <td>justinkieferwx</td>\n",
       "      <td>2018-10-10 12:48:33+00:00</td>\n",
       "      <td>#Michael | live streaming coverage on http://f...</td>\n",
       "      <td>#Michael #panhandlestrong</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3787</th>\n",
       "      <td>1049993102814011392</td>\n",
       "      <td>Juanyehthomas1</td>\n",
       "      <td>2018-10-10 12:00:38+00:00</td>\n",
       "      <td>I pray my people back home in the panhandle st...</td>\n",
       "      <td>#PanhandleStrong</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3788</th>\n",
       "      <td>1049849561542479872</td>\n",
       "      <td>legacyonthebay</td>\n",
       "      <td>2018-10-10 02:30:15+00:00</td>\n",
       "      <td>Our area is prepared and ready for action! #pa...</td>\n",
       "      <td>#panhandlestrong</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3789</th>\n",
       "      <td>1049746029501960192</td>\n",
       "      <td>PSCSoftball</td>\n",
       "      <td>2018-10-09 19:38:51+00:00</td>\n",
       "      <td>All @PSCSoftball softball ladies are safe and ...</td>\n",
       "      <td>#PanhandleStrong</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3658 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id         username                      date  \\\n",
       "0     1051623671951974400        GulfPower 2018-10-14 23:59:56+00:00   \n",
       "1     1051623669363965952  Postcards4Potus 2018-10-14 23:59:55+00:00   \n",
       "2     1051623651320184832     LauraHKByrne 2018-10-14 23:59:51+00:00   \n",
       "3     1051623649197875201      SupplierCom 2018-10-14 23:59:50+00:00   \n",
       "4     1051623615911927808   Heart_to_Heart 2018-10-14 23:59:42+00:00   \n",
       "...                   ...              ...                       ...   \n",
       "3785  1050007027676835842     valerie__lyn 2018-10-10 12:55:58+00:00   \n",
       "3786  1050005160351666176   justinkieferwx 2018-10-10 12:48:33+00:00   \n",
       "3787  1049993102814011392   Juanyehthomas1 2018-10-10 12:00:38+00:00   \n",
       "3788  1049849561542479872   legacyonthebay 2018-10-10 02:30:15+00:00   \n",
       "3789  1049746029501960192      PSCSoftball 2018-10-09 19:38:51+00:00   \n",
       "\n",
       "                                                   text  \\\n",
       "0     â€œWe are pleased to be making steady progress a...   \n",
       "1     @realDonaldTrump really doesn't care! Seriousl...   \n",
       "2     This is an excellent point. The best and easie...   \n",
       "3     For those affected by #HurricaneMichael member...   \n",
       "4     The devastation from #HurricaneMichael is hard...   \n",
       "...                                                 ...   \n",
       "3785  Praying for my home and everyone in the panhan...   \n",
       "3786  #Michael | live streaming coverage on http://f...   \n",
       "3787  I pray my people back home in the panhandle st...   \n",
       "3788  Our area is prepared and ready for action! #pa...   \n",
       "3789  All @PSCSoftball softball ladies are safe and ...   \n",
       "\n",
       "                                               hashtags geo  \n",
       "0                                     #HurricaneMichael      \n",
       "1                                     #HurricaneMichael      \n",
       "2                                     #HurricaneMichael      \n",
       "3                                     #HurricaneMichael      \n",
       "4                #HurricaneMichael #PanamaCity #Florida      \n",
       "...                                                 ...  ..  \n",
       "3785  #FloridaStrong #PanhandleStrong #HurricaneMichael      \n",
       "3786                          #Michael #panhandlestrong      \n",
       "3787                                   #PanhandleStrong      \n",
       "3788                                   #panhandlestrong      \n",
       "3789                                   #PanhandleStrong      \n",
       "\n",
       "[3658 rows x 6 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking out the DF \n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll add an aditional column to this dataframe, since these tweets are pulled via search word, we'll assign a 0 to the traffic column to idicate the tweets did not come specifically from a traffic twitter account. Then we'll write the data to a csv for data cleaning and modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating new column, assigning value 0 \n",
    "test['traffic'] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writting data to CSV, commented out as to not re-write my data. \n",
    "#test.to_csv('../datasets/searched_tweets.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll gather data from the twitter users who report on traffic. The function below has alot of similarities with the previous one, but this one is searching based off usernames instead of key words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_accounts(search_username ,start_date, end_date, tweets_per_search, is_loc = True):\n",
    "    tweet_list = []\n",
    "    if is_loc:\n",
    "        for word in search_username: \n",
    "            tweetCriteria = got.manager.TweetCriteria().setUsername(search_username)\\\n",
    "                                                       .setSince(start_date)\\\n",
    "                                                       .setUntil(end_date)\\\n",
    "                                                       .setMaxTweets(tweets_per_search)\\\n",
    "\n",
    "            tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "            time.sleep(2)\n",
    "\n",
    "            for tweet in tweets:\n",
    "                tweet_list.append(create_tweet(tweet))\n",
    "        return pd.DataFrame(tweet_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a list of twitter usernames that you'd like to search. The usernames that we used to pull data are: @fl511_panhand1, 511 for the Florida panhandle; @BayCountyTMC,  Bay County Traffic Management Center; and @WJHG_TV, a Northwest Florida news station. We'll run through the same processing steps as the previous data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of usernames to search\n",
    "usernames = ['BayCountyTMC' , \"fl511_panhandl\" , 'WJHG_TV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pulling the tweets \n",
    "traffic_tweets =get_tweets_accounts(usernames, '2018-10-01' , '2018-10-15', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2076, 6)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking out the shape \n",
    "traffic_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping duplicate tweets \n",
    "traffic_tweets.drop_duplicates(subset = 'id' , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(692, 6)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calling shape again, there were a lot of duplicate tweets in this pull. \n",
    "traffic_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a categorical system to identify which search this tweet came from. \n",
    "traffic_tweets['traffic'] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writting to CSV for working data. \n",
    "#traffic_tweets.to_csv('../datasets/account_tweets.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Twitter's free API, and the python library Tweepy, allow us to pull data from the specific keywords and from specific usernames for the past 7 days. We'll feed our same search parameters from our historic tweets search for the current tweets as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step for pulling tweets via Tweepy is to access our API keys, then we'll authenticate our connection via Tweepy. To do this on your own, you'll need to apply for your Twitter developer keys. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in our hidden keys \n",
    "ENV = pd.read_json(\"../env.json\", typ = 'series')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning keys and tokens to variables, here is where you'd enter your own Twitter API keys \n",
    "API_KEY= ENV[\"API key\"]\n",
    "API_SECRET_KEY = ENV[\"API secret key\"]\n",
    "ACCESS_TOKEN = ENV[\"Access token\"]\n",
    "ACCESS_TOKEN_SECRET = ENV['Access token secret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#authenticating via tweepy our API access. \n",
    "auth = tweepy.OAuthHandler(API_KEY , API_SECRET_KEY)\n",
    "auth.set_access_token(ACCESS_TOKEN , ACCESS_TOKEN_SECRET)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we created a function to pull the tweets from the specific usernames, using the Tweepy wrapper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created a function that pull the most recent 200 tweets from the twitter user who's handle you feed into the function.  \n",
    "# This was adapted from a previous group : https://github.com/DCapella/evacuation-routes/blob/master/code/The_Function_H.Toumy_X.Sin'Austin.ipynb\n",
    "\n",
    "def gather_tweets(handle:str, n=200): # takes twitter username , and number of tweets to return, 200 is the max. \n",
    "    tweets_everything = api.user_timeline(handle, count = n)     #using the tweepy API to pull the data\n",
    "    df = pd.DataFrame(columns = ['id', 'tweets', 'date', 'location'])  #empty DF the tweets will eventually go into \n",
    "    for i in tweets_everything: # for each tweet \n",
    "        tweets = i.text         #text of tweet \n",
    "        try: \n",
    "            date = i.formatted_date     #trying to find date by calling formated_date, but if that doesn't work \n",
    "        except: \n",
    "            date = i.created_at     #pull when the tweet was created_at \n",
    "        try:\n",
    "            location = i.geo['coordinates'] #next, we'll check to make sure there aren't any attached geo data\n",
    "        except: \n",
    "            try: \n",
    "                location = i.coordinates  # if theres no stored geo data, does the tweet itself have coordinates.\n",
    "            except: \n",
    "                location = 'NaN'   #finally, fill with NAN if we can recover no location data. \n",
    "                \n",
    "        tweet_id = i.id     # tweet id assigned \n",
    "        df.loc[len(df)] = [tweet_id, tweets, date, location]  #creating DF \n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then used that function to pull the most recent tweets from our 3 previously searched traffic and news accounts: @BayCountyTMC , @fl511_panhandl, and @WJHG_TV. We then concated the data to a single DataFrame for export. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running the function on our first Twitter user then creating a username column in the DF, checking out the first few rows\n",
    "traffic1 = gather_tweets('BayCountyTMC')\n",
    "traffic1['username'] = 'BayCountyTMC'\n",
    "traffic1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running function to pull our second twitter user tweets, adding a username column. \n",
    "traffic2 = gather_tweets('fl511_panhandl')\n",
    "traffic2['username']= 'fl511_panhandl'\n",
    "traffic2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pulling final twitter users tweets from the past 7 days. \n",
    "traffic3 = gather_tweets('WJHG_TV')\n",
    "traffic3['username'] = 'WJHG_TV'\n",
    "traffic3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 5)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concating that DF to be exported and used as testing data. \n",
    "live_traffic = pd.concat(objs = [traffic1, traffic2, traffic3],\n",
    "                       axis= 0)\n",
    "\n",
    "live_traffic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing data to CSV, commented out to ensure we don't rewrite. \n",
    "#live_traffic.to_csv('../datasets/live_traffic.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there created the function to pull key search words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created function that pulls search data via Tweepy. again this was adapted from the same code above. \n",
    "def gather_tweets(q , n=200):   #search query, number of tweets to return\n",
    "    tweets_everything = api.search(q , count = n)   #using Tweepy API for the search \n",
    "    df = pd.DataFrame(columns = ['id', 'tweets', 'date', 'location']) #creating the df for the tweets. \n",
    "    \n",
    "    for i in tweets_everything:  #for each tweet \n",
    "        tweets = i.text      #text of tweet \n",
    "        try: \n",
    "            date = i.formatted_date   #trying to find the date by calling, fornatted_date, but if doesn't work\n",
    "        except: \n",
    "            date = i.created_at      #pull when the tweet was created_at \n",
    "        try:\n",
    "            location = i.geo['coordinates']  #next, we'll check to make sure there aren't any attached geo data\n",
    "        except: \n",
    "            try: \n",
    "                location = i.coordinates  #if there is no stored geo data, is there coordinates attatched to the tweet\n",
    "            except: \n",
    "                location = 'NaN'  #fill values where no coordinate info was found as NAN\n",
    "                \n",
    "        tweet_id = i.id           #tweet ID \n",
    "                \n",
    "        df.loc[len(df)] = [tweet_id, tweets, date, location] # looping through tweets and adding them to DF\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hurricanemichael',\n",
       " 'hurricane',\n",
       " 'panamacitybeach',\n",
       " 'panamacityflorida',\n",
       " '850strong',\n",
       " 'gulfpower',\n",
       " 'baycounty',\n",
       " 'floridastrong',\n",
       " 'panhandlestrong']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#as a reminder, here are our search words \n",
    "search_words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there we use the function to pull the recent twitter data for our search words and concat them to a single DataFrame for export. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we're running the list of words through the function \n",
    "hurricane_michael = gather_tweets('hurricanemichael')\n",
    "hurricane = gather_tweets('hurricane')\n",
    "panama_city_beach = gather_tweets('panamacitybeach')\n",
    "panama_city_florida = gather_tweets('panamacityflorida')\n",
    "strong = gather_tweets('850strong')\n",
    "gulfpower = gather_tweets('gulfpower')\n",
    "baycounty = gather_tweets('baycounty')\n",
    "florida_strong = gather_tweets('floridastrong')\n",
    "panhandle_strong = gather_tweets('panhandlestrong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concating our search words DF into a single DF \n",
    "live_search = pd.concat( objs = [hurricane_michael, hurricane, panama_city_beach,\n",
    "                                 panama_city_florida, strong, gulfpower,\n",
    "                                 baycounty, florida_strong, panhandle_strong], axis = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(422, 4)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding the shape of the data\n",
    "live_search.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writting to CSV \n",
    "#live_search.to_csv('../datasets/live_search.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Previous Group Repository](https://github.com/DCapella/evacuation-routes)  \n",
    "[GetOldTweets3 Documentation](https://pypi.org/project/GetOldTweets3/)  \n",
    "[Tweepy Documentation](http://www.tweepy.org/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
