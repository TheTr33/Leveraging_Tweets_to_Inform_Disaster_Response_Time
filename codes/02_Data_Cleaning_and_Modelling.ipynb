{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-20T00:46:09.752Z"
    }
   },
   "source": [
    "# Data EDA and Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Imports](#Imports)<br>\n",
    "* [Preparing the Data](#Preparing-the-Data)<br> \n",
    "    * [Costructing the Corpus](#Constructing-the-Corpus)<br>\n",
    "    * [Corpus Cleaning](#Corpus-Cleaning)<br>\n",
    "* [Model Preparation](#Model-Preparation)<br>\n",
    "* [Limitations & Conclusions](#Limitations-&-Conclusions)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T15:18:42.453703Z",
     "start_time": "2020-02-26T15:18:37.727343Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sreer\\AppData\\LocalContinuum\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\sreer\\AppData\\LocalContinuum\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.bagging module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\sreer\\AppData\\LocalContinuum\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\sreer\\AppData\\LocalContinuum\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\sreer\\AppData\\LocalContinuum\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\sreer\\AppData\\LocalContinuum\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category = FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Constructing the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read in the historic tweets from the `BayCountTMC`, `fl511_panhandl`, and `WJHG_TV` in the `account_tweets.csv` and unfiltered tweets from the `searched_tweets.csv` scraped using the GetOldTweets3 library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T15:18:42.519527Z",
     "start_time": "2020-02-26T15:18:42.454702Z"
    }
   },
   "outputs": [],
   "source": [
    "account = pd.read_csv('../datasets/account_tweets.csv')\n",
    "searched = pd.read_csv('../datasets/searched_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `account` dataframe contains the tweets that we will classify as traffic incident or emergency related tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T15:18:42.546458Z",
     "start_time": "2020-02-26T15:18:42.521523Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>username</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>geo</th>\n",
       "      <th>traffic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1051600805755846657</td>\n",
       "      <td>WJHG_TV</td>\n",
       "      <td>2018-10-14 22:29:04+00:00</td>\n",
       "      <td>A lot of disaster assistance info... as well a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1051595380369096704</td>\n",
       "      <td>fl511_panhandl</td>\n",
       "      <td>2018-10-14 22:07:31+00:00</td>\n",
       "      <td>Cleared: Traffic congestion in Bay on US-231 s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1051580211702243333</td>\n",
       "      <td>WJHG_TV</td>\n",
       "      <td>2018-10-14 21:07:14+00:00</td>\n",
       "      <td>Jessica and Ryan are about to handle our storm...</td>\n",
       "      <td>#wjhgmichael</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1051575221671682054</td>\n",
       "      <td>fl511_panhandl</td>\n",
       "      <td>2018-10-14 20:47:24+00:00</td>\n",
       "      <td>Cleared: Object on roadway in Okaloosa on I-10...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1051574997964255232</td>\n",
       "      <td>fl511_panhandl</td>\n",
       "      <td>2018-10-14 20:46:31+00:00</td>\n",
       "      <td>New: Object on roadway in Okaloosa on I-10 wes...</td>\n",
       "      <td>#fl511</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id        username                       date  \\\n",
       "0  1051600805755846657         WJHG_TV  2018-10-14 22:29:04+00:00   \n",
       "1  1051595380369096704  fl511_panhandl  2018-10-14 22:07:31+00:00   \n",
       "2  1051580211702243333         WJHG_TV  2018-10-14 21:07:14+00:00   \n",
       "3  1051575221671682054  fl511_panhandl  2018-10-14 20:47:24+00:00   \n",
       "4  1051574997964255232  fl511_panhandl  2018-10-14 20:46:31+00:00   \n",
       "\n",
       "                                                text      hashtags  geo  \\\n",
       "0  A lot of disaster assistance info... as well a...           NaN  NaN   \n",
       "1  Cleared: Traffic congestion in Bay on US-231 s...           NaN  NaN   \n",
       "2  Jessica and Ryan are about to handle our storm...  #wjhgmichael  NaN   \n",
       "3  Cleared: Object on roadway in Okaloosa on I-10...           NaN  NaN   \n",
       "4  New: Object on roadway in Okaloosa on I-10 wes...        #fl511  NaN   \n",
       "\n",
       "   traffic  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "account.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `searched` dataframe contains tweets that have been queried using the same search words but without particular accounts specified. Tweets from here will represent our negative class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T15:18:42.560417Z",
     "start_time": "2020-02-26T15:18:42.549453Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>username</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>geo</th>\n",
       "      <th>traffic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1051623671951974400</td>\n",
       "      <td>GulfPower</td>\n",
       "      <td>2018-10-14 23:59:56+00:00</td>\n",
       "      <td>“We are pleased to be making steady progress a...</td>\n",
       "      <td>#HurricaneMichael</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1051623669363965952</td>\n",
       "      <td>Postcards4Potus</td>\n",
       "      <td>2018-10-14 23:59:55+00:00</td>\n",
       "      <td>@realDonaldTrump really doesn't care! Seriousl...</td>\n",
       "      <td>#HurricaneMichael</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1051623651320184832</td>\n",
       "      <td>LauraHKByrne</td>\n",
       "      <td>2018-10-14 23:59:51+00:00</td>\n",
       "      <td>This is an excellent point. The best and easie...</td>\n",
       "      <td>#HurricaneMichael</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1051623649197875201</td>\n",
       "      <td>SupplierCom</td>\n",
       "      <td>2018-10-14 23:59:50+00:00</td>\n",
       "      <td>For those affected by #HurricaneMichael member...</td>\n",
       "      <td>#HurricaneMichael</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1051623615911927808</td>\n",
       "      <td>Heart_to_Heart</td>\n",
       "      <td>2018-10-14 23:59:42+00:00</td>\n",
       "      <td>The devastation from #HurricaneMichael is hard...</td>\n",
       "      <td>#HurricaneMichael #PanamaCity #Florida</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id         username                       date  \\\n",
       "0  1051623671951974400        GulfPower  2018-10-14 23:59:56+00:00   \n",
       "1  1051623669363965952  Postcards4Potus  2018-10-14 23:59:55+00:00   \n",
       "2  1051623651320184832     LauraHKByrne  2018-10-14 23:59:51+00:00   \n",
       "3  1051623649197875201      SupplierCom  2018-10-14 23:59:50+00:00   \n",
       "4  1051623615911927808   Heart_to_Heart  2018-10-14 23:59:42+00:00   \n",
       "\n",
       "                                                text  \\\n",
       "0  “We are pleased to be making steady progress a...   \n",
       "1  @realDonaldTrump really doesn't care! Seriousl...   \n",
       "2  This is an excellent point. The best and easie...   \n",
       "3  For those affected by #HurricaneMichael member...   \n",
       "4  The devastation from #HurricaneMichael is hard...   \n",
       "\n",
       "                                 hashtags  geo  traffic  \n",
       "0                       #HurricaneMichael  NaN        0  \n",
       "1                       #HurricaneMichael  NaN        0  \n",
       "2                       #HurricaneMichael  NaN        0  \n",
       "3                       #HurricaneMichael  NaN        0  \n",
       "4  #HurricaneMichael #PanamaCity #Florida  NaN        0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searched.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that both of the scrapes have been read in, they will be concatenated into one dataframe containing both the positive and negative class so that it can be further cleaned and implemented in modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T15:18:42.568397Z",
     "start_time": "2020-02-26T15:18:42.562413Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([account,searched],axis = 0, ignore_index= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the two files into a new notebook it is important to check once again for any blank rows that have been converted to `NaN` values since exporting the scrapes into `.csv` files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T15:18:42.578370Z",
     "start_time": "2020-02-26T15:18:42.569396Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "username       0\n",
       "date           0\n",
       "text           4\n",
       "hashtags    1394\n",
       "geo         4348\n",
       "traffic        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 null values are present in the text column post export. Since there are only 4 missing values and it is text data, the rows containing the null values can safely be dropped without worrying about any adverse effects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T15:18:42.588344Z",
     "start_time": "2020-02-26T15:18:42.579369Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(df.loc[df['text'].isnull()==True].index,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T15:18:42.597321Z",
     "start_time": "2020-02-26T15:18:42.590339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "username       0\n",
       "date           0\n",
       "text           0\n",
       "hashtags    1390\n",
       "geo         4344\n",
       "traffic        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that there are no null values left in the text column and the traffic column, which will be our corpus and our classification variable respectively, we can move on to the next steps of the data cleaning process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Corpus Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to feed the model the most meaningful words, the raw text data of the tweets will need to be trimmed down. First, links will need to be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T15:18:42.603303Z",
     "start_time": "2020-02-26T15:18:42.599314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A lot of disaster assistance info... as well as shelters, food/water PODS, and curfew information can be found here... https://www.floridadisaster.org/info/\n"
     ]
    }
   ],
   "source": [
    "# Test tweet with https link in it\n",
    "print(df['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T15:18:42.643196Z",
     "start_time": "2020-02-26T15:18:42.605298Z"
    }
   },
   "outputs": [],
   "source": [
    "# Removes https links\n",
    "df['text'] = df['text'].str.replace('http\\S+|www.\\S+', ' ', case = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T15:18:42.648183Z",
     "start_time": "2020-02-26T15:18:42.644194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A lot of disaster assistance info... as well as shelters, food/water PODS, and curfew information can be found here...  \n"
     ]
    }
   ],
   "source": [
    "# link has been removed\n",
    "print(df['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, all tweets will undergo some generalized cleaning including removing non alphabet characters, converting all letters to lowercase and removing typically meaningless words to a model by removing stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T15:18:42.655164Z",
     "start_time": "2020-02-26T15:18:42.649181Z"
    }
   },
   "outputs": [],
   "source": [
    "def tweet_cleaning(raw):\n",
    "    # 1. Remove non-letters.\n",
    "    letters_only = re.sub('[^a-zA-Z]', ' ', raw)\n",
    "    \n",
    "    # 2. Convert to lower case, split into individual words.\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    # 3. Join all the stopwords as a string with \" \", remove \"'\" from the stopwords and split it as a list.\n",
    "    stops = \" \".join(stopwords.words('english')).replace(\"'\", \"\").split()\n",
    "    \n",
    "    # 4. Remove stopwords.\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # 5. Join the words back into one string separated by space and return the result.\n",
    "    return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T15:18:44.106845Z",
     "start_time": "2020-02-26T15:18:42.656163Z"
    }
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(tweet_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T15:18:44.118800Z",
     "start_time": "2020-02-26T15:18:44.107830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lot disaster assistance info well shelters food water pods curfew information found'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T15:18:44.131782Z",
     "start_time": "2020-02-26T15:18:44.121793Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatizing(tweet):\n",
    "    \n",
    "    # 1. Split into individual words.\n",
    "    words = tweet.split()\n",
    "    \n",
    "    # 2. Remove stopwords.\n",
    "    stops = \" \".join(stopwords.words('english')).replace(\"'\",\"\").split()\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # 3. Lemmatizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatizing = [lemmatizer.lemmatize(i) for i in meaningful_words]\n",
    "    \n",
    "    # 4. Join the words back into one string separated by space and return the result.\n",
    "    return(\" \".join(lemmatizing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T15:18:46.965722Z",
     "start_time": "2020-02-26T15:18:44.135755Z"
    }
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lemmatizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T15:18:46.971677Z",
     "start_time": "2020-02-26T15:18:46.966692Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lot disaster assistance info well shelter food water pod curfew information found'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to modeling it is important to establish a baseline to compare our models accuracy scores to. This will be done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T15:18:47.057552Z",
     "start_time": "2020-02-26T15:18:46.973197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.8407\n",
       "1    0.1593\n",
       "Name: traffic, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline accuracy score\n",
    "df['traffic'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that with an 84% baseline that there is a strong class imbalance in the data as a result of our account specific tweet scrape resulting in less tweets than the scrape from the `searched_tweets.csv` file. Therefore, bootstrapping the data will be necessary in our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T15:18:47.065506Z",
     "start_time": "2020-02-26T15:18:47.058524Z"
    }
   },
   "outputs": [],
   "source": [
    "y = df['traffic'] # classifier variable\n",
    "X = df['text'] # corpus\n",
    "\n",
    "X_train,X_test, y_train,y_test = train_test_split(X,y,stratify = y, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the `traffic` column as the classification variable, and the `text` column for the corpus. The following gridsearch will test whether the CountVectorizer or the TfidfVectorizer would be better suited for our classification model over a random forest classifier, an adaboost classification model, and a support vector machine for classification. Parameters for the vectorizers include the list of builtin English stop_words from sklearn in order to account for any difference in the stopwords list used in the lemmatizing function from nltk. Additionally, monogram, bigram, and trigram ranges will be used in order to account for specific combinations of words such as \"hurricane michael\". The random forest, and adaboost models for classification were selected because the data is automatically implemented in each decision tree in the model. The support vector machine for classification is also used in the gridsearch because when using the any sort of word vectorizer, each unique word will be counted as an independent feature in the model. It is safe to assume that there will be numerous unique words used in the corpus of tweets which will cause our dataset to have a large number of independent features and therefore significantly high dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T15:18:47.075479Z",
     "start_time": "2020-02-26T15:18:47.066504Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiating Pipelines for potential models\n",
    "pipe_rf_cvec = Pipeline([('cvec',CountVectorizer()),                         \n",
    "                         ('rf', RandomForestClassifier(n_estimators= 100))])\n",
    "pipe_rf_tfidf = Pipeline([('tfidf', TfidfVectorizer()),                          \n",
    "                          ('rf', RandomForestClassifier(n_estimators=100))])\n",
    "pipe_ada_cvec = Pipeline([('cvec', CountVectorizer()),                          \n",
    "                          ('ada', AdaBoostClassifier(n_estimators=100))])\n",
    "pipe_ada_tfidf = Pipeline([('tfidf', TfidfVectorizer()),                           \n",
    "                           ('ada', AdaBoostClassifier(n_estimators = 100))])\n",
    "pipe_svc_cvec = Pipeline([('cvec', CountVectorizer()),\n",
    "                           ('svc', SVC(gamma = 'auto',\n",
    "                                       random_state = 42))])\n",
    "pipe_svc_tfidf = Pipeline([('tfidf', TfidfVectorizer()),                    \n",
    "                           ('svc', SVC(gamma = 'auto',\n",
    "                                       random_state = 42))])\n",
    "# Instantiating vectorizer parameters\n",
    "cvec_params = {'cvec__stop_words':[None,'english'],\n",
    "               'cvec__ngram_range':[(1,1),(1,2),(1,3)]}\n",
    "tfidf_params = {'tfidf__stop_words': [None, 'english'],\n",
    "                'tfidf__ngram_range': [(1,1),(1,2),(1,3)]}\n",
    "\n",
    "# Random Forest GridSearches\n",
    "grid_rf_cvec = GridSearchCV(pipe_rf_cvec, cvec_params,cv = 5)\n",
    "grid_rf_tfidf = GridSearchCV(pipe_rf_tfidf, tfidf_params, cv = 5)\n",
    "\n",
    "\n",
    "# Adaboost GridSearches\n",
    "grid_ada_cvec = GridSearchCV(pipe_ada_cvec, cvec_params, cv = 5)\n",
    "grid_ada_tfidf = GridSearchCV(pipe_ada_tfidf, tfidf_params, cv = 5)\n",
    "\n",
    "# SVC GridSearch\n",
    "grid_svc_cvec = GridSearchCV(pipe_svc_cvec,cvec_params, cv = 5)\n",
    "grid_svc_tfidf = GridSearchCV(pipe_svc_tfidf, tfidf_params, cv = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T15:21:22.300727Z",
     "start_time": "2020-02-26T15:18:47.076476Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorized Random Forest\n",
      "best params: {'cvec__ngram_range': (1, 1), 'cvec__stop_words': 'english'}\n",
      "best estimator train score: 1.0\n",
      "best estimator test score: 0.9475138121546961\n",
      "\n",
      "TFIDF Random Forest\n",
      "best params: {'tfidf__ngram_range': (1, 1), 'tfidf__stop_words': 'english'}\n",
      "best estimator train score: 1.0\n",
      "best estimator test score: 0.9410681399631676\n",
      "\n",
      "CountVectorized Adaboost\n",
      "best params: {'cvec__ngram_range': (1, 1), 'cvec__stop_words': None}\n",
      "best estimator train score: 0.9904849600982197\n",
      "best estimator test score: 0.9732965009208103\n",
      "\n",
      "TFIDF Adaboost\n",
      "best params: {'tfidf__ngram_range': (1, 2), 'tfidf__stop_words': None}\n",
      "best estimator train score: 0.994475138121547\n",
      "best estimator test score: 0.9650092081031307\n",
      "\n",
      "CountVectorized SVC\n",
      "best params: {'cvec__ngram_range': (1, 1), 'cvec__stop_words': None}\n",
      "best estimator train score: 0.8406998158379374\n",
      "best estimator test score: 0.8406998158379374\n",
      "\n",
      "TFIDF SVC\n",
      "best params: {'tfidf__ngram_range': (1, 1), 'tfidf__stop_words': None}\n",
      "best estimator train score: 0.8406998158379374\n",
      "best estimator test score: 0.8406998158379374\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [grid_rf_cvec, grid_rf_tfidf, grid_ada_cvec, \n",
    "          grid_ada_tfidf, grid_svc_cvec, grid_svc_tfidf]\n",
    "model_names = ['CountVectorized Random Forest','TFIDF Random Forest',\n",
    "               'CountVectorized Adaboost','TFIDF Adaboost', 'CountVectorized SVC',\n",
    "               'TFIDF SVC']\n",
    "\n",
    "# loops through each gridsearch and prints out accuracy scores and parameters for the best estimator\n",
    "for (model, model_name) in zip(models, model_names):\n",
    "    model.fit(X_train,y_train)\n",
    "    print(f'{model_name}')\n",
    "    print(f'best params: {model.best_params_}')\n",
    "    print(f'best estimator train score: {model.best_estimator_.score(X_train,y_train)}')\n",
    "    print(f'best estimator test score: {model.best_estimator_.score(X_test,y_test)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the models performed better than the baseline and show no signs of overfitting except for the support vector machine for classification which actually underperformed the baseline by .01%. The parameters used for the model were generally the default values but this score could be improved upon adding increased regularization, a better kernel method, and finetuning the rest of the parameters. The adaboost model achieved the best test score and training score with nominal bias variance tradeoff. The support vector machine for classification had the lowest variance between testing and training data for both the CountVectorized set and the TfidfVectorized set but due to the default parameters for `gamma`(auto). Below I will test the effect of the training and testing score of the SVC if the `gamma` is changed to scale which will allow the model to better capture the complexity of the data by taking into account the variance of the independent variables as opposed to just the inverse of the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T15:21:22.311698Z",
     "start_time": "2020-02-26T15:21:22.301755Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiating Pipelines for potential models\n",
    "pipe_svc_cvec = Pipeline([('cvec', CountVectorizer()),\n",
    "                           ('svc', SVC(gamma = 'scale',\n",
    "                                       random_state = 42))])\n",
    "pipe_svc_tfidf = Pipeline([('tfidf', TfidfVectorizer()),                    \n",
    "                           ('svc', SVC(gamma = 'scale',\n",
    "                                       random_state = 42))])\n",
    "# Instantiating vectorizer parameters\n",
    "cvec_params = {'cvec__stop_words':[None,'english'],\n",
    "               'cvec__ngram_range':[(1,1),(1,2),(1,3)]}\n",
    "tfidf_params = {'tfidf__stop_words': [None, 'english'],\n",
    "                'tfidf__ngram_range': [(1,1),(1,2),(1,3)]}\n",
    "\n",
    "# SVC GridSearch\n",
    "grid_svc_cvec = GridSearchCV(pipe_svc_cvec,cvec_params, cv = 5)\n",
    "grid_svc_tfidf = GridSearchCV(pipe_svc_tfidf, tfidf_params, cv = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T15:22:31.781065Z",
     "start_time": "2020-02-26T15:21:22.317192Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorized SVC\n",
      "best params: {'cvec__ngram_range': (1, 1), 'cvec__stop_words': None}\n",
      "best estimator train score: 0.9953959484346224\n",
      "best estimator test score: 0.9511970534069981\n",
      "\n",
      "TFIDF SVC\n",
      "best params: {'tfidf__ngram_range': (1, 1), 'tfidf__stop_words': 'english'}\n",
      "best estimator train score: 0.9978514426028238\n",
      "best estimator test score: 0.9429097605893186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [grid_svc_cvec, grid_svc_tfidf] # trimmed down to only run SVC grid searches\n",
    "model_names = ['CountVectorized SVC', 'TFIDF SVC'] # trimmed down to only run SVC grid searches\n",
    "\n",
    "for (model, model_name) in zip(models, model_names):\n",
    "    \n",
    "    model.fit(X_train,y_train)\n",
    "    print(f'{model_name}')\n",
    "    print(f'best params: {model.best_params_}')\n",
    "    print(f'best estimator train score: {model.best_estimator_.score(X_train,y_train)}')\n",
    "    print(f'best estimator test score: {model.best_estimator_.score(X_test,y_test)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After changing the `gamma` method to 'scale', the both the CountVectorized SVC and the TFIDF SVC improved beyod the baseline. However, the adaboost models still yielded the highest accuracy scores for the test data and showed less variance and was therefore better fit to the data than the support vector machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T15:22:31.881794Z",
     "start_time": "2020-02-26T15:22:31.783059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false pos:4, false neg:25\n",
      "true pos: 148, true neg: 909\n",
      "sensitivity: 0.8554913294797688, specificity: 0.9956188389923329\n",
      "false_neg_rate: 0.14450867052023122\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix for Count Vectorized Adaboost\n",
    "ada_test_preds = grid_ada_cvec.best_estimator_.predict(X_test)\n",
    "\n",
    "tn,fp,fn,tp = confusion_matrix(y_test,ada_test_preds).ravel()\n",
    "sens = tp/(tp+fn)\n",
    "spec = tn/(tn+fp)\n",
    "acc = (tp+tn)/(tn+tp+fp+fn)\n",
    "false_pos_rate = fn/(tp+fn)\n",
    "print(f'false pos:{fp}, false neg:{fn}')\n",
    "print(f'true pos: {tp}, true neg: {tn}')\n",
    "print(f'sensitivity: {sens}, specificity: {spec}')\n",
    "print(f'false_neg_rate: {false_pos_rate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T15:22:32.005467Z",
     "start_time": "2020-02-26T15:22:31.885790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false pos:8, false neg:30\n",
      "true pos: 143, true neg: 905\n",
      "sensitivity: 0.8265895953757225, specificity: 0.9912376779846659\n",
      "false_neg_rate: 0.17341040462427745\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix for Tfidf Vectorized Adaboost\n",
    "ada_test_preds = grid_ada_tfidf.best_estimator_.predict(X_test)\n",
    "\n",
    "tn,fp,fn,tp = confusion_matrix(y_test,ada_test_preds).ravel()\n",
    "sens = tp/(tp+fn)\n",
    "spec = tn/(tn+fp)\n",
    "acc = (tp+tn)/(tn+tp+fp+fn)\n",
    "false_pos_rate = fn/(tp+fn)\n",
    "print(f'false pos:{fp}, false neg:{fn}')\n",
    "print(f'true pos: {tp}, true neg: {tn}')\n",
    "print(f'sensitivity: {sens}, specificity: {spec}')\n",
    "print(f'false_neg_rate: {false_pos_rate}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity and specificity are also within nominal ranges for the CountVectorized adaboost and the TfidfVectorized adaboost. Upon comparing them to the sensitivity and specificity of the support vector machine..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T15:22:32.253834Z",
     "start_time": "2020-02-26T15:22:32.006463Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false pos:1, false neg:52\n",
      "true pos: 121, true neg: 912\n",
      "sensitivity: 0.6994219653179191, specificity: 0.9989047097480832\n",
      "false_neg_rate: 0.30057803468208094\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix for Count Vectorized SVC\n",
    "svc_test_preds = grid_svc_cvec.best_estimator_.predict(X_test)\n",
    "\n",
    "tn,fp,fn,tp = confusion_matrix(y_test,svc_test_preds).ravel()\n",
    "sens = tp/(tp+fn)\n",
    "spec = tn/(tn+fp)\n",
    "acc = (tp+tn)/(tn+tp+fp+fn)\n",
    "false_pos_rate = fn/(tp+fn)\n",
    "print(f'false pos:{fp}, false neg:{fn}')\n",
    "print(f'true pos: {tp}, true neg: {tn}')\n",
    "print(f'sensitivity: {sens}, specificity: {spec}')\n",
    "print(f'false_neg_rate: {false_pos_rate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T15:22:32.516132Z",
     "start_time": "2020-02-26T15:22:32.254798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false pos:0, false neg:62\n",
      "true pos: 111, true neg: 913\n",
      "sensitivity: 0.6416184971098265, specificity: 1.0\n",
      "false_neg_rate: 0.3583815028901734\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix for Tfidf SVC\n",
    "svc_test_preds = grid_svc_tfidf.best_estimator_.predict(X_test)\n",
    "\n",
    "tn,fp,fn,tp = confusion_matrix(y_test,svc_test_preds).ravel()\n",
    "sens = tp/(tp+fn)\n",
    "spec = tn/(tn+fp)\n",
    "acc = (tp+tn)/(tn+tp+fp+fn)\n",
    "false_pos_rate = fn/(tp+fn)\n",
    "print(f'false pos:{fp}, false neg:{fn}')\n",
    "print(f'true pos: {tp}, true neg: {tn}')\n",
    "print(f'sensitivity: {sens}, specificity: {spec}')\n",
    "print(f'false_neg_rate: {false_pos_rate}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... it is clear that the CountVectorized adaboost model is better for the classification as the model will be better at properly classifying traffic incidents and emergencies and minimizing false negatives compared to the support vector machine. Therefore, the CountVectorized adaboost model will be the one used to classify tweets when using live twitter data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitations & Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to our classification assumptions of the dataset being that the positive class is any tweet scraped from the BayCountTMC, fl511_panhandl, and WJHG_TV accounts, the model is technically built to classify between tweets from those accounts and tweets not from those accounts. Therefore, the model may not perform well on live tweets which will consist of tweets from all sorts of accounts that won't have the specific language used in the preformatted announcements and tweets from the accounts we deemed as the positive class.<br>\n",
    "Despite this, the model that will be used to classify the live tweets for this project will be an adaboost model with a countvectorized corpus after cleaning lemmatization and stopwords from the nltk package removed, with a monogram range, and 100 bootstrapped trees `n_estimators`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
